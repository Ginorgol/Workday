{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T22:09:20.391062Z",
     "start_time": "2019-04-15T22:09:03.914034Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T22:09:24.528761Z",
     "start_time": "2019-04-15T22:09:24.123201Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./largest.csv', encoding='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T22:09:26.757286Z",
     "start_time": "2019-04-15T22:09:26.687595Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T22:09:27.169363Z",
     "start_time": "2019-04-15T22:09:27.136672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Assigned_Group</th>\n",
       "      <th>ClosureProductName</th>\n",
       "      <th>IncidentProduct</th>\n",
       "      <th>Description</th>\n",
       "      <th>Custom_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ITOPS-PLAN-L1</td>\n",
       "      <td>GLOVIA DAO/ASL (32681)</td>\n",
       "      <td>GLOVIA DAO/ASL (32681)</td>\n",
       "      <td>#Relief - Order missing relief in Glovia - Cos...</td>\n",
       "      <td>L1 App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ITOPS-TOOLS-L1</td>\n",
       "      <td>ORACLE DATABASE</td>\n",
       "      <td>ORACLE DATABASE</td>\n",
       "      <td>Request to remove the below Mentioned targets ...</td>\n",
       "      <td>L1 App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ITOPS-IEO ITRC Monitoring Network-Data</td>\n",
       "      <td>NETWORK DATA DEVICE</td>\n",
       "      <td>NETWORK DATA DEVICE</td>\n",
       "      <td>Network Outage_-_         _-_LCJ1MFGETL2K2</td>\n",
       "      <td>IT Ops L1 Monitoring Response Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ITOPS-IEO ITRC Monitoring Database</td>\n",
       "      <td>Oracle Enterprise Manager (32850)</td>\n",
       "      <td>Oracle Enterprise Manager (32850)</td>\n",
       "      <td>Target Availability_-_Status - / -ausfloem2db0...</td>\n",
       "      <td>IT Ops L1 Monitoring Response Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ITOPS-IEO ITRC Monitoring Database</td>\n",
       "      <td>SmartPrice (391181)</td>\n",
       "      <td>SmartPrice (391181)</td>\n",
       "      <td>Target Availability_-_Status - / -ausplsmphadb...</td>\n",
       "      <td>IT Ops L1 Monitoring Response Center</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Assigned_Group                 ClosureProductName  \\\n",
       "0                           ITOPS-PLAN-L1             GLOVIA DAO/ASL (32681)   \n",
       "1                          ITOPS-TOOLS-L1                    ORACLE DATABASE   \n",
       "2  ITOPS-IEO ITRC Monitoring Network-Data                NETWORK DATA DEVICE   \n",
       "3      ITOPS-IEO ITRC Monitoring Database  Oracle Enterprise Manager (32850)   \n",
       "4      ITOPS-IEO ITRC Monitoring Database                SmartPrice (391181)   \n",
       "\n",
       "                     IncidentProduct  \\\n",
       "0             GLOVIA DAO/ASL (32681)   \n",
       "1                    ORACLE DATABASE   \n",
       "2                NETWORK DATA DEVICE   \n",
       "3  Oracle Enterprise Manager (32850)   \n",
       "4                SmartPrice (391181)   \n",
       "\n",
       "                                         Description  \\\n",
       "0  #Relief - Order missing relief in Glovia - Cos...   \n",
       "1  Request to remove the below Mentioned targets ...   \n",
       "2         Network Outage_-_         _-_LCJ1MFGETL2K2   \n",
       "3  Target Availability_-_Status - / -ausfloem2db0...   \n",
       "4  Target Availability_-_Status - / -ausplsmphadb...   \n",
       "\n",
       "                        Custom_category  \n",
       "0                                L1 App  \n",
       "1                                L1 App  \n",
       "2  IT Ops L1 Monitoring Response Center  \n",
       "3  IT Ops L1 Monitoring Response Center  \n",
       "4  IT Ops L1 Monitoring Response Center  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T22:09:31.899869Z",
     "start_time": "2019-04-15T22:09:31.887277Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "# <br /><br /> ==> after the above steps, we are getting \"br br\"\n",
    "# we are including them into stop words list\n",
    "# instead of <br /> if we have <br/> these tags would have revmoved in the 1st step\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "# Combining all the above stundents \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T22:10:31.668674Z",
     "start_time": "2019-04-15T22:09:33.206790Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156351/156351 [00:58<00:00, 2675.41it/s]\n"
     ]
    }
   ],
   "source": [
    "short_desc = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(df['Description'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    short_desc.append(sentance.strip())\n",
    "\n",
    "df['cleaned'] = short_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:16:31.599560Z",
     "start_time": "2019-04-15T21:16:31.574030Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df['cleaned'], df['Assigned_Group'],\n",
    "                                                    shuffle=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:16:35.953900Z",
     "start_time": "2019-04-15T21:16:31.600557Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some sample features(unique words in the corpus) ['aa', 'aa cc', 'aa config', 'aaci', 'aaci dont', 'aalnskdkaeavk', 'aaplication', 'aarons', 'aarthi', 'aarthi ravivarman']\n",
      "==================================================\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer  (125080, 95280)\n",
      "the number of unique words including both unigrams and bigrams  95280\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "tf_idf_vect.fit(X_train)\n",
    "print(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\n",
    "print('='*50)\n",
    "\n",
    "X_train_tf_idf = tf_idf_vect.transform(X_train)\n",
    "print(\"the type of count vectorizer \",type(X_train_tf_idf))\n",
    "print(\"the shape of out text TFIDF vectorizer \",X_train_tf_idf.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", X_train_tf_idf.get_shape()[1])\n",
    "\n",
    "X_test_tfidf = tf_idf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:17:14.255391Z",
     "start_time": "2019-04-15T21:17:13.894029Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('tf_idf_vect_assigned_group_150k.pickle', 'wb') as f:\n",
    "    pickle.dump(tf_idf_vect, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:31:56.169359Z",
     "start_time": "2019-04-15T21:17:22.512246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7341626427041028"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier()\n",
    "clf.fit(X_train_tf_idf, Y_train)\n",
    "clf.score(X_test_tfidf, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:31:56.222107Z",
     "start_time": "2019-04-15T21:31:56.171465Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('clf_assigned_group_150k.pickle', 'wb') as f:\n",
    "    pickle.dump(clf, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:34:39.031164Z",
     "start_time": "2019-04-15T21:34:39.018654Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1888.48it/s]\n"
     ]
    }
   ],
   "source": [
    "review_text = [\"mail\"]\n",
    "\n",
    "cleaned_review_text = []\n",
    "for sentance in tqdm(review_text):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    cleaned_review_text.append(sentance.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:34:39.329901Z",
     "start_time": "2019-04-15T21:34:39.323876Z"
    }
   },
   "outputs": [],
   "source": [
    "z = tf_idf_vect.transform(cleaned_review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T21:34:39.650437Z",
     "start_time": "2019-04-15T21:34:39.603495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ITOPS-SALES-L1'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T22:10:31.692782Z",
     "start_time": "2019-04-15T22:10:31.671770Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df['cleaned'], df['ClosureProductName'], shuffle=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-15T22:10:36.080482Z",
     "start_time": "2019-04-15T22:10:31.694709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some sample features(unique words in the corpus) ['aa', 'aa cc', 'aa config', 'aaci', 'aaci dont', 'aalnskdkaeavk', 'aaplication', 'aarons', 'aarthi', 'aarthi ravivarman']\n",
      "==================================================\n",
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text TFIDF vectorizer  (125080, 95280)\n",
      "the number of unique words including both unigrams and bigrams  95280\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "tf_idf_vect.fit(X_train)\n",
    "print(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\n",
    "print('='*50)\n",
    "\n",
    "X_train_tf_idf = tf_idf_vect.transform(X_train)\n",
    "print(\"the type of count vectorizer \",type(X_train_tf_idf))\n",
    "print(\"the shape of out text TFIDF vectorizer \",X_train_tf_idf.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", X_train_tf_idf.get_shape()[1])\n",
    "\n",
    "X_test_tfidf = tf_idf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-15T22:11:15.881Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train_tf_idf, Y_train)\n",
    "model_lr.score(X_test_tfidf, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
